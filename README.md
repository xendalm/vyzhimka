# rusumm

This repository contains code and experiments for evaluating the impact of synthetic data and training methods on the quality of Russian abstractive text summarization using various evaluation metrics.

# Training and Evaluation overview

Models:
- FRED-T5-large (used for fine-tuning on summarization task and DPO)

Data:
- Open-source Russian summarization datasets (MLSUM, Gazeta, XLSum, WikiLingua, etc.)
- Synthetic data (instruction, input, summary pairs) generated using Gemini 2.0

Evaluation:
- Standard auto-evaluation metrics (Rouge, BLEU, BERTScore, etc.)
- LLM-as-a-judge evaluation
- Natural Language Inference evaluation
- Reward model trained on the SEAHORSE human feedback dataset

# Code

- .
- .

# Results


